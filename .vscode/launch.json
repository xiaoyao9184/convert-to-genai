{
    "version": "0.2.0",
    "configurations": [
        {
            "name": "gradio: run app",
            "type": "debugpy",
            "request": "launch",
            "program": "${workspaceFolder}/gradio/run.py",
            "console": "integratedTerminal",
            "cwd": "${workspaceFolder}",
            "args": [
                "${workspaceFolder}/gradio/app.py"
            ],
            "env": {
                // "HF_ENDPOINT": "https://huggingface.co",
                "HF_TOKEN": "${env:HF_TOKEN}",
                "SPACE_AUTHOR_NAME": "xiaoyao9184",
                "HUGGINGFACE_HUB_CACHE": "${env:HOME}/.cache/huggingface/hub",
                "OUTPUT_DIR": "${env:HOME}/.genai-build/models/",
                "IGNORE_CONVERTED": "true",
                "IGNORE_ERRORS": "true"
            },
            "justMyCode": false
        },
        {
            "name": "onnxruntime_genai: build cpu-int4-rtn-block-32",
            "type": "debugpy",
            "request": "launch",
            "module": "onnxruntime_genai.models.builder",
            "console": "integratedTerminal",
            "args": [
                "-m", "Qwen/Qwen2.5-0.5B-Instruct",
                "-o", "$HOME/.genai-build/models/convert-to-genai/Qwen2.5-0.5B-Instruct/cpu_and_mobile/cpu-int4-rtn-block-32",
                "-p", "int4",
                "-e", "cpu",
                "-c", "${env:HOME}/.cache/huggingface/hub",
                "--extra_options", "hf_token=false", "filename=Qwen2.5-0.5B-Instruct-cpu-int4-rtn-block-32.onnx",
                    "int4_algo_config=rtn", "int4_block_size=32", "int4_block_size=32"
            ],
            "justMyCode": false
        }
    ]
}